{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wPyu65GneeCs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "CUDA: 12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA:\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d28a8d5b3f746dbbae6816c67f7d035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463a4246ff66464ea44842c45f24f89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test_sft-00000-of-00001.parquet:   0%|          | 0.00/48.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731ecf2f59f0478583cc3a884d0921b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_sft-00000-of-00002.parquet:   0%|          | 0.00/217M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cd03039efd4d638d45546d96f4eead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_sft-00001-of-00002.parquet:   0%|          | 0.00/217M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47acdb1bb10445279888cb72eac31d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_sft split:   0%|          | 0/21424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a94b8b28ba4c4580b8e94c26c4f849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_sft split:   0%|          | 0/192598 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042b58ff4bd94f5ea0636f3be9687fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d2d78da9134628b67a0d1f5323755e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "simplification.jsonl:   0%|          | 0.00/2.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f439941f000d4b85b77e90bf891e0b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2867757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultrachat rows: 1000\n",
      "Leesplank rows: 1000\n",
      "Ultrachat keys: dict_keys(['prompt', 'prompt_id', 'messages'])\n",
      "Leesplank keys: dict_keys(['prompt', 'result'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ultrachat = load_dataset(\n",
    "    \"BramVanroy/ultrachat_200k_dutch\",\n",
    "    split=\"train_sft[:1000]\"\n",
    ")\n",
    "\n",
    "leesplank = load_dataset(\n",
    "    \"UWV/Leesplank_NL_wikipedia_simplifications\",\n",
    "    split=\"train[:1000]\"\n",
    ")\n",
    "\n",
    "print(\"Ultrachat rows:\", len(ultrachat))\n",
    "print(\"Leesplank rows:\", len(leesplank))\n",
    "print(\"Ultrachat keys:\", ultrachat[0].keys())\n",
    "print(\"Leesplank keys:\", leesplank[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultrachat usable rows: 1000\n",
      "Leesplank usable rows: 1000\n",
      "Wrote: /content/drive/MyDrive/LLM/data/dutch_tutor/train_v2_mix.jsonl\n",
      "Rows written: 1400\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are a Dutch language tutor. \"\n",
    "    \"Reply in simple English. \"\n",
    "    \"Correct the Dutch sentence, explain briefly, then give 2 short Dutch examples.\"\n",
    ")\n",
    "\n",
    "out_path = Path(\"/content/drive/MyDrive/LLM/data/dutch_tutor/train_v2_mix.jsonl\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def normalize_ws(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def write_row(f, user_text: str, assistant_text: str):\n",
    "    row = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_text},\n",
    "        ]\n",
    "    }\n",
    "    f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# --- 1) Ultrachat: keep as conversation SFT (Dutch). ---\n",
    "def ultrachat_to_rows(example):\n",
    "    # example[\"messages\"] is already in chat format; we wrap with our system message\n",
    "    msgs = example[\"messages\"]\n",
    "    # Find the first user and first assistant turn to keep it simple\n",
    "    user = next((m[\"content\"] for m in msgs if m.get(\"role\") == \"user\" and m.get(\"content\")), None)\n",
    "    assistant = next((m[\"content\"] for m in msgs if m.get(\"role\") == \"assistant\" and m.get(\"content\")), None)\n",
    "    if not user or not assistant:\n",
    "        return None\n",
    "    user = normalize_ws(user)\n",
    "    assistant = normalize_ws(assistant)\n",
    "    return (user, assistant)\n",
    "\n",
    "# --- 2) Leesplank: convert \"prompt/result\" into a tutor-style simplification task. ---\n",
    "def leesplank_to_rows(example):\n",
    "    prompt = example.get(\"prompt\", \"\")\n",
    "    result = example.get(\"result\", \"\")\n",
    "    if not prompt or not result:\n",
    "        return None\n",
    "    prompt = normalize_ws(prompt)\n",
    "    result = normalize_ws(result)\n",
    "\n",
    "    user = (\n",
    "        \"Simplify this Dutch text for an A2 learner. \"\n",
    "        \"Keep the meaning. Use short sentences.\\n\\n\"\n",
    "        f\"Text:\\n{prompt}\"\n",
    "    )\n",
    "    assistant = (\n",
    "        f\"Simplified version:\\n{result}\\n\\n\"\n",
    "        \"Explanation (simple English):\\n\"\n",
    "        \"I used shorter sentences and simpler words.\\n\"\n",
    "        \"Examples:\\n\"\n",
    "        \"- Dit is een kort voorbeeld.\\n\"\n",
    "        \"- Nog een eenvoudig voorbeeld.\\n\"\n",
    "    )\n",
    "    return (user, assistant)\n",
    "\n",
    "# Build the mix: 80% ultrachat, 20% leesplank (from the 1000-sample subsets you loaded)\n",
    "ultra_rows = []\n",
    "for ex in ultrachat:\n",
    "    row = ultrachat_to_rows(ex)\n",
    "    if row:\n",
    "        ultra_rows.append(row)\n",
    "\n",
    "lees_rows = []\n",
    "for ex in leesplank:\n",
    "    row = leesplank_to_rows(ex)\n",
    "    if row:\n",
    "        lees_rows.append(row)\n",
    "\n",
    "print(\"Ultrachat usable rows:\", len(ultra_rows))\n",
    "print(\"Leesplank usable rows:\", len(lees_rows))\n",
    "\n",
    "# Mix and write\n",
    "random.seed(42)\n",
    "target_total = 2000  # small v2 for now (safe). Increase later.\n",
    "n_ultra = int(target_total * 0.8)\n",
    "n_lees = target_total - n_ultra\n",
    "\n",
    "sample_ultra = random.sample(ultra_rows, min(n_ultra, len(ultra_rows)))\n",
    "sample_lees = random.sample(lees_rows, min(n_lees, len(lees_rows)))\n",
    "\n",
    "mixed = sample_ultra + sample_lees\n",
    "random.shuffle(mixed)\n",
    "\n",
    "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for user_text, assistant_text in mixed:\n",
    "        write_row(f, user_text, assistant_text)\n",
    "\n",
    "print(\"Wrote:\", out_path)\n",
    "print(\"Rows written:\", len(mixed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c944b23770444cf99267af7fef198848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1400\n",
      "system : You are a Dutch language tutor. Reply in simple English. Correct the Dutch sente ...\n",
      "user : Zou u wellicht aanbevelingen kunnen doen met betrekking tot de kruidenafstelling ...\n",
      "assistant : Voor een authentieke Cajun draai aan uw vleespotpastai, zou ik aanbevelen om uw  ...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_v2 = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/content/drive/MyDrive/LLM/data/dutch_tutor/train_v2_mix.jsonl\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "print(\"Rows:\", len(ds_v2))\n",
    "print(ds_v2[0][\"messages\"][0][\"role\"], \":\", ds_v2[0][\"messages\"][0][\"content\"][:80], \"...\")\n",
    "print(ds_v2[0][\"messages\"][1][\"role\"], \":\", ds_v2[0][\"messages\"][1][\"content\"][:80], \"...\")\n",
    "print(ds_v2[0][\"messages\"][2][\"role\"], \":\", ds_v2[0][\"messages\"][2][\"content\"][:80], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -q -U trl bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnb_config ready\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"bnb_config ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_config ready\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"lora_config ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d030a6e2df48b1b74ed19d93359b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daaeba07d00741138ec520699876e081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a1bdfb1b7949669c1399083909195a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [350/350 18:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.934300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.846600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.862900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.787300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.798800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.765700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.779600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.770700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.762600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.783200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.810700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.846000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.767100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.808000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN V3 DONE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "model_id = \"BramVanroy/GEITje-7B-ultra\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "model_v3 = get_peft_model(base_model, lora_config)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/LLM/models/dutch_tutor_lora_v3\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    report_to=[],\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model_v3,\n",
    "    train_dataset=ds_v2,\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"TRAIN V3 DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Files: ['README.md', 'checkpoint-200', 'checkpoint-350']\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "\n",
    "path = \"/content/drive/MyDrive/LLM/models/dutch_tutor_lora_v3\"\n",
    "print(\"Exists:\", os.path.exists(path))\n",
    "print(\"Files:\", sorted([os.path.basename(p) for p in glob.glob(path + \"/*\")])[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint: /content/drive/MyDrive/LLM/models/dutch_tutor_lora_v3/checkpoint-350\n",
      "Copied: ['adapter_config.json', 'adapter_model.safetensors']\n",
      "Final path: /content/drive/MyDrive/LLM/models/dutch_tutor_lora_v3_final\n",
      "Final files: ['adapter_config.json', 'adapter_model.safetensors']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "src = \"/content/drive/MyDrive/LLM/models/dutch_tutor_lora_v3\"\n",
    "dst = \"/content/drive/MyDrive/LLM/models/dutch_tutor_lora_v3_final\"\n",
    "Path(dst).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# pick the latest checkpoint\n",
    "ckpts = sorted(glob.glob(os.path.join(src, \"checkpoint-*\")), key=lambda p: int(p.split(\"-\")[-1]))\n",
    "latest = ckpts[-1]\n",
    "print(\"Latest checkpoint:\", latest)\n",
    "\n",
    "# copy adapter files\n",
    "copied = []\n",
    "for name in [\"adapter_config.json\", \"adapter_model.safetensors\", \"adapter_model.bin\"]:\n",
    "    f = os.path.join(latest, name)\n",
    "    if os.path.exists(f):\n",
    "        shutil.copy2(f, os.path.join(dst, name))\n",
    "        copied.append(name)\n",
    "\n",
    "print(\"Copied:\", copied)\n",
    "print(\"Final path:\", dst)\n",
    "print(\"Final files:\", sorted(os.listdir(dst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9a86ef64f146288706d60b15f303b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Correct: Ik ben gisteren naar de winkel gegaan.\n",
      "- Explanation: 'Heb' is not used with movement verbs in the past. Dutch uses 'ben gegaan'.\n",
      "- Examples:\n",
      "  - Ik ben naar de supermarkt gegaan.\n",
      "  - Ik ben gisteren naar huis gegaan.\n",
      "\n",
      "Now do the same.\n",
      "\n",
      "Student sentence: Ik heb gisteren naar winkel gaan.\n",
      "Answer:\n",
      "- Correct: Ik ben gisteren naar de winkel gegaan.\n",
      "- Explanation: 'Heb' is not used with movement verbs in the past. Dutch uses 'ben gegaan'.\n",
      "- Examples:\n",
      "  - Ik ben naar de supermarkt gegaan.\n",
      "  - Ik ben gisteren naar huis gegaan.\n",
      "\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are a Dutch language tutor.\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"- Reply in simple English.\\n\"\n",
    "    \"- Correct the Dutch sentence.\\n\"\n",
    "    \"- Explain the mistake briefly.\\n\"\n",
    "    \"- Give 2 short Dutch examples.\\n\"\n",
    ")\n",
    "\n",
    "def build_prompt(sentence):\n",
    "     return f\"\"\"\n",
    "You are a Dutch language tutor.\n",
    "Reply in simple English.\n",
    "\n",
    "Example:\n",
    "Student sentence: Ik heb gisteren naar winkel gaan.\n",
    "Answer:\n",
    "- Correct: Ik ben gisteren naar de winkel gegaan.\n",
    "- Explanation: 'Heb' is not used with movement verbs in the past. Dutch uses 'ben gegaan'.\n",
    "- Examples:\n",
    "  - Ik ben naar de supermarkt gegaan.\n",
    "  - Ik ben gisteren naar huis gegaan.\n",
    "\n",
    "Now do the same.\n",
    "\n",
    "Student sentence: {sentence}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "base_model = \"BramVanroy/GEITje-7B-ultra\"\n",
    "adapter_path = \"/content/drive/MyDrive/LLM/models/dutch_tutor_lora_v3_final\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "model.eval()\n",
    "\n",
    "prompt = build_prompt(\"Ik heb gisteren naar winkel gaan.\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=180,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "if \"Answer:\" in text:\n",
    "    text = text.split(\"Answer:\", 1)[-1].strip()\n",
    "\n",
    "print(text)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMh6hOZFRhC2gOms3xtFdkH",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
