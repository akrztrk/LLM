{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"toc_visible":true,"machine_shape":"hm"}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">  Hands-On LangChain for LLM Applications Development: Chatbots Memory </center>\n","\n","***\n","\n","\n","When interacting with language models, such as Chatbots, the absence of memory poses a significant hurdle in creating natural and seamless conversations. Users expect continuity and context retention, which traditional models lack. This limitation becomes particularly evident in applications where ongoing dialogue is crucial for user engagement and satisfaction.\n","\n","LangChain offers robust solutions to address this challenge. Memory, in this context, refers to the ability of the language model to remember previous parts of a conversation and use that information to inform subsequent interactions. By incorporating memory into the model’s architecture, LangChain enables Chatbots and similar applications to maintain a conversational flow that mimics human-like dialogue.\n","\n","LangChain’s memory capabilities extend beyond mere recall of past interactions. It encompasses sophisticated mechanisms for storing, organizing, and retrieving relevant information, ensuring that the Chatbot can respond appropriately based on the context of the conversation. This not only enhances the user experience but also enables the Chatbot to provide more accurate and relevant responses over time.\n","\n","#### <a id=\"top\"></a>\n","# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b>Table of contents</b></div>\n","\n","<div style=\"background-color: rgba(60, 121, 245, 0.03); padding:30px; font-size:15px; font-family: consolas;\">\n","<ul>\n","    <li><a href=\"#1\" target=\"_self\" rel=\" noreferrer nofollow\">1. Setting Up Working Environment & Getting Starting </a> </li>\n","    <li><a href=\"#2\" target=\"_self\" rel=\" noreferrer nofollow\">2. Conversation Buffer Memory </a></li>\n","    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">3. Conversation Buffer Window Memory </a></li>\n","    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">4. Conversation Token Buffer Memory </a></li>\n","    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">5. Conversation Summary Memory </a></li>\n","</ul>\n","</div>\n","\n","***\n","\n"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"KzKMrPJ5Mc3k"}},{"cell_type":"markdown","source":["<a id=\"1\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 1. Setting Up Working Environment & Getting Started </b></div>\n","\n","To get started we are going to import OS, import OpenAI, and load my OpenAI secret key. If you’re running this locally, and you don’t have OpenAI installed yet, you might need to run pip to install OpenAI."],"metadata":{"id":"Hf0gENmoMc3n"}},{"cell_type":"code","source":["!pip install langchain\n","!pip install langchain_community\n","!pip install openai"],"metadata":{"id":"FWtLhsN5NHDh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import openai\n","from openai import OpenAI\n","from getpass import getpass\n","\n","OPENAI_API_KEY =  getpass('Please enter your open_api_key:')\n","api_key = {\n","    \"OPENAI_API_KEY\":OPENAI_API_KEY\n","}\n","locals().update(api_key)\n","os.environ.update(api_key)\n","\n","llm_model = \"gpt-3.5-turbo\""],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:46:05.922131Z","iopub.execute_input":"2024-05-18T07:46:05.923081Z","iopub.status.idle":"2024-05-18T07:46:52.902329Z","shell.execute_reply.started":"2024-05-18T07:46:05.923045Z","shell.execute_reply":"2024-05-18T07:46:52.901317Z"},"trusted":true,"id":"FEEF-FLYMc3o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"2\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 2. Conversation Buffer Memory </b></div>\n","\n","\n","Let’s start with a motivating example for memory, using LangChain to manage a chat or a chatbot conversation. I am going to set the LLM as a chat interface of OpenAI with a temperature equal to 0. We will use the memory as a ConversationBufferMemory and then build a conversation chain."],"metadata":{"id":"uvAK4CLlMc3p"}},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","from langchain.chains import ConversationChain\n","from langchain.memory import ConversationBufferMemory\n","\n","llm = ChatOpenAI(temperature=0.0, model=llm_model, openai_api_key=OPENAI_API_KEY)\n","memory = ConversationBufferMemory()\n","conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:50:39.403443Z","iopub.execute_input":"2024-05-18T07:50:39.40414Z","iopub.status.idle":"2024-05-18T07:50:39.439666Z","shell.execute_reply.started":"2024-05-18T07:50:39.404105Z","shell.execute_reply":"2024-05-18T07:50:39.438752Z"},"trusted":true,"id":"M_kE1ZKEMc3p","executionInfo":{"status":"ok","timestamp":1716312746477,"user_tz":-120,"elapsed":204,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Let's start a conversation using conversation.predict() and the given input will be “Hi, my name is Youssef” and let’s see the response."],"metadata":{"id":"nvNJNlXbMc3q"}},{"cell_type":"code","source":["conversation.predict(input=\"Hi, my name is Akar\")"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:51:04.294733Z","iopub.execute_input":"2024-05-18T07:51:04.29509Z","iopub.status.idle":"2024-05-18T07:51:05.200851Z","shell.execute_reply.started":"2024-05-18T07:51:04.295063Z","shell.execute_reply":"2024-05-18T07:51:05.199826Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":264},"id":"54aPboktMc3q","executionInfo":{"status":"ok","timestamp":1716312757141,"user_tz":-120,"elapsed":1347,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"5c73fb93-c3d3-42ca-d3a2-f4dc5fb13d1a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","\n","Human: Hi, my name is Akar\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Hello Akar! It's nice to meet you. How can I assist you today?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Then, let’s ask it, what is 1 plus 1? 1 plus 1 is 2, and then ask it again, you know, what’s my name?"],"metadata":{"id":"L9ytGQBbMc3q"}},{"cell_type":"code","source":["conversation.predict(input=\"What is 3*3?\")"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:51:49.18206Z","iopub.execute_input":"2024-05-18T07:51:49.182827Z","iopub.status.idle":"2024-05-18T07:51:50.099916Z","shell.execute_reply.started":"2024-05-18T07:51:49.182791Z","shell.execute_reply":"2024-05-18T07:51:50.09882Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"8oo-TjPxMc3q","executionInfo":{"status":"ok","timestamp":1716312917192,"user_tz":-120,"elapsed":1093,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"f81f77c5-b945-4e95-ee58-20b46e183d1d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you. How can I assist you today?\n","Human: What is 3*3?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'3 multiplied by 3 is equal to 9. Is there anything else you would like to know?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Now we will ask it again, “What’s my name?” and you can see its response “Your name is Youssef, as you mentioned earlier”."],"metadata":{"id":"JtvHqm-jMc3r"}},{"cell_type":"code","source":["conversation.predict(input=\"What is my name?\")\n"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:52:13.890842Z","iopub.execute_input":"2024-05-18T07:52:13.89123Z","iopub.status.idle":"2024-05-18T07:52:15.70993Z","shell.execute_reply.started":"2024-05-18T07:52:13.891198Z","shell.execute_reply":"2024-05-18T07:52:15.708913Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":316},"id":"DVaVIvC0Mc3r","executionInfo":{"status":"ok","timestamp":1716312925430,"user_tz":-120,"elapsed":780,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"f0ef0bf0-f853-4a4e-f5fc-0c992b49419d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you. How can I assist you today?\n","Human: What is 3*3?\n","AI: 3 multiplied by 3 is equal to 9. Is there anything else you would like to know?\n","Human: What is my name?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'Your name is Akar.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["So this is a prompt that LangChain has generated to have the system have a hopeful and friendly conversation with you. When you execute this on the second and third parts of the conversations, it keeps the prompt as follows.\n","\n","Since I have used the memory variable to store the memory. So if I were to print memory.buffer, it has stored the conversation so far."],"metadata":{"id":"MWlLKxZ1Mc3r"}},{"cell_type":"code","source":["print(memory.buffer)\n"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:52:54.399121Z","iopub.execute_input":"2024-05-18T07:52:54.399624Z","iopub.status.idle":"2024-05-18T07:52:54.405435Z","shell.execute_reply.started":"2024-05-18T07:52:54.399569Z","shell.execute_reply":"2024-05-18T07:52:54.404319Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"MCbJhYstMc3r","executionInfo":{"status":"ok","timestamp":1716312948778,"user_tz":-120,"elapsed":249,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"4ec3e161-cfa9-4eb0-9ccc-f6b90cedd548"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you. How can I assist you today?\n","Human: What is 3*3?\n","AI: 3 multiplied by 3 is equal to 9. Is there anything else you would like to know?\n","Human: What is my name?\n","AI: Your name is Akar.\n"]}]},{"cell_type":"markdown","source":["You can also print this out using the memory.loadMemoryVariables({}). The curly braces here are an empty dictionary. There are some more advanced features that you can use with a more sophisticated input, but it is not covered in this article but you can find it in LangChain documentation. So don’t worry about why there’s an empty curly braces here. But this is what LangChain has remembered in the memory of the conversation so far. It’s just everything that the AI or the human has said."],"metadata":{"id":"QS53AiJYMc3r"}},{"cell_type":"code","source":["memory.load_memory_variables({})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:53:29.799839Z","iopub.execute_input":"2024-05-18T07:53:29.800213Z","iopub.status.idle":"2024-05-18T07:53:29.807565Z","shell.execute_reply.started":"2024-05-18T07:53:29.800184Z","shell.execute_reply":"2024-05-18T07:53:29.806446Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"NskDm10AMc3s","executionInfo":{"status":"ok","timestamp":1716312956630,"user_tz":-120,"elapsed":245,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"5393ea09-ba33-4d9d-cb83-cd145c76b8ae"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': \"Human: Hi, my name is Akar\\nAI: Hello Akar! It's nice to meet you. How can I assist you today?\\nHuman: What is 3*3?\\nAI: 3 multiplied by 3 is equal to 9. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Akar.\"}"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["The way that LangChain is storing the conversation is with this ConversationBufferMemory. If I were to use the ConversationBufferMemory to specify a couple of inputs and outputs, this is how you add new things to the memory if you wish to do so explicitly."],"metadata":{"id":"uArqPkhpMc3s"}},{"cell_type":"code","source":["memory = ConversationBufferMemory()\n","memory.save_context({\"input\": \"Hi\"},\n","                    {\"output\": \"What's up\"})\n"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:54:16.2189Z","iopub.execute_input":"2024-05-18T07:54:16.2193Z","iopub.status.idle":"2024-05-18T07:54:16.224751Z","shell.execute_reply.started":"2024-05-18T07:54:16.219269Z","shell.execute_reply":"2024-05-18T07:54:16.223648Z"},"trusted":true,"id":"cH6AlTbAMc3s","executionInfo":{"status":"ok","timestamp":1716312962664,"user_tz":-120,"elapsed":207,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Memory.saveContext says, hi, what’s up? I know this is not the most exciting conversation, but I wanted to give a short example. And with that, this is what the status of the memory is. Once again, let me show the memory variables. Now, if you want to add additional data to the memory, you can keep on saving additional context. So, the conversation goes on, not much, just hanging, cool."],"metadata":{"id":"rDhkpzg_Mc3s"}},{"cell_type":"code","source":["print(memory.buffer)"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:05:07.071108Z","iopub.execute_input":"2024-05-18T08:05:07.071515Z","iopub.status.idle":"2024-05-18T08:05:07.077079Z","shell.execute_reply.started":"2024-05-18T08:05:07.071486Z","shell.execute_reply":"2024-05-18T08:05:07.075969Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"LwcB3CmzMc3s","executionInfo":{"status":"ok","timestamp":1716312965671,"user_tz":-120,"elapsed":202,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"4407887d-185d-4ebd-87a4-0a3f8fd8e30f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Human: Hi\n","AI: What's up\n"]}]},{"cell_type":"markdown","source":["When you use a large language model for a chat conversation, the large language model itself is stateless. The language model itself does not remember the conversation you’ve had so far. Therefore each call to the API endpoint is independent. Chatbots have memory only because there’s usually rapid code that provides the full conversation that’s been had so far as context to the LLM. Therefore the memory can store explicitly the terms or the utterances so far. Hi, my name is Youssef. Hello, it’s just nice to meet you and so on.\n","\n","This memory storage is used as input or additional context to the LLM so that it can generate output as if it’s just having the next conversational turn, knowing what’s been said before.\n","\n","As the conversation becomes long, the amount of memory needed becomes long, and the cost of sending a lot of tokens to the LLM, which usually charges based on the number of tokens it needs to process, will also become more expensive.\n","\n","So LangChain provides several convenient kinds of memory to store and accumulate the conversation. So far, we’ve been looking at the ConversationBufferMemory. Let’s look at a different type of memory."],"metadata":{"id":"YuUCc7dQMc3s"}},{"cell_type":"markdown","source":["<a id=\"3\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 3. Conversation Buffer Window Memory</b></div>\n","\n","Let's start with importing the conversation buffer window memory that only keeps a window of memory. If I set memory to conversational buffer window memory with k equals one, the variable k =1 specifies that I want to remember just one conversational exchange. That is one utterance from me and one utterance from a chatbot.\n","\n"],"metadata":{"id":"cyRdCsxmMc3s"}},{"cell_type":"code","source":["from langchain.memory import ConversationBufferWindowMemory\n","memory = ConversationBufferWindowMemory(k=1)\n","memory.load_memory_variables({})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:05:53.796262Z","iopub.execute_input":"2024-05-18T08:05:53.796637Z","iopub.status.idle":"2024-05-18T08:05:53.80395Z","shell.execute_reply.started":"2024-05-18T08:05:53.796597Z","shell.execute_reply":"2024-05-18T08:05:53.8031Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"bEEpPsgIMc3s","executionInfo":{"status":"ok","timestamp":1716313172282,"user_tz":-120,"elapsed":255,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"7f048189-0c07-4e03-bda0-23728cc9643c"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': ''}"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["If I were to have it save context, “Hi, what’s up, not much, just hanging”. If I were to look at memory.load_memory_variables({}), it only remembers the most recent utterance as you can see below."],"metadata":{"id":"XmeGwdFOMc3t"}},{"cell_type":"code","source":["memory.save_context({\"input\": \"Hi\"},\n","                    {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})\n","memory.load_memory_variables({})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:06:21.292913Z","iopub.execute_input":"2024-05-18T08:06:21.293337Z","iopub.status.idle":"2024-05-18T08:06:21.301785Z","shell.execute_reply.started":"2024-05-18T08:06:21.293304Z","shell.execute_reply":"2024-05-18T08:06:21.300502Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"f3mGvu2ZMc3t","executionInfo":{"status":"ok","timestamp":1716313175829,"user_tz":-120,"elapsed":291,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"dc6702b6-2bf5-4df0-d449-2227d1c4c9ee"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'Human: Not much, just hanging\\nAI: Cool'}"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["It only returns the recent conversation because k was equal to one. So this is a nice feature because it lets you keep track of just the most recent few conversational terms. In practice, you probably won’t use this with k equals one. You use this with k set to a larger number.\n","\n"," If I were to rerun the conversation that we had above, we would say “Hi, my name is Youssef”. Then we will say “What is 1 plus 1?”. Finally, I will ask it “What is my name?”. Because we set k equals 1, it only remembers the last exchange versus what is 1 plus 1? The answer is 1 plus 1 equals 2, and it’s forgotten this early exchange which is now, now says, sorry, don’t have access to that information."],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:06:31.602687Z","iopub.execute_input":"2024-05-18T08:06:31.60337Z","iopub.status.idle":"2024-05-18T08:06:31.61302Z","shell.execute_reply.started":"2024-05-18T08:06:31.603334Z","shell.execute_reply":"2024-05-18T08:06:31.611658Z"},"id":"5o24uBNCMc3t"}},{"cell_type":"code","source":["conversation.predict(input=\"Hi, my name is Akar\")"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:07:44.147044Z","iopub.execute_input":"2024-05-18T08:07:44.147454Z","iopub.status.idle":"2024-05-18T08:07:44.863981Z","shell.execute_reply.started":"2024-05-18T08:07:44.14742Z","shell.execute_reply":"2024-05-18T08:07:44.862943Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":351},"id":"U5VrjYNAMc3t","executionInfo":{"status":"ok","timestamp":1716313191106,"user_tz":-120,"elapsed":1035,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"2fa8cb8d-ad91-463c-bc36-69e6c3d44c50"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you. How can I assist you today?\n","Human: What is 3*3?\n","AI: 3 multiplied by 3 is equal to 9. Is there anything else you would like to know?\n","Human: What is my name?\n","AI: Your name is Akar.\n","Human: Hi, my name is Akar\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Hello Akar! It's nice to meet you again. How can I assist you today?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["conversation.predict(input=\"What is 3*3?\")"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:07:52.683022Z","iopub.execute_input":"2024-05-18T08:07:52.683442Z","iopub.status.idle":"2024-05-18T08:07:53.424795Z","shell.execute_reply.started":"2024-05-18T08:07:52.68341Z","shell.execute_reply":"2024-05-18T08:07:53.423629Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":385},"id":"XUkpWnV8Mc3t","executionInfo":{"status":"ok","timestamp":1716313194329,"user_tz":-120,"elapsed":830,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"ee24d285-bb42-4216-c17b-f952ddc61f57"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you. How can I assist you today?\n","Human: What is 3*3?\n","AI: 3 multiplied by 3 is equal to 9. Is there anything else you would like to know?\n","Human: What is my name?\n","AI: Your name is Akar.\n","Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you again. How can I assist you today?\n","Human: What is 3*3?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'3 multiplied by 3 is equal to 9. Is there anything else you would like to know?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["conversation.predict(input=\"What is my name?\")"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:08:01.124116Z","iopub.execute_input":"2024-05-18T08:08:01.124961Z","iopub.status.idle":"2024-05-18T08:08:01.951135Z","shell.execute_reply.started":"2024-05-18T08:08:01.124918Z","shell.execute_reply":"2024-05-18T08:08:01.95015Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":420},"id":"p9K0cmwdMc3t","executionInfo":{"status":"ok","timestamp":1716313197068,"user_tz":-120,"elapsed":820,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"c33b9cac-4ad5-4b48-ef69-e735d93f5ca2"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you. How can I assist you today?\n","Human: What is 3*3?\n","AI: 3 multiplied by 3 is equal to 9. Is there anything else you would like to know?\n","Human: What is my name?\n","AI: Your name is Akar.\n","Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you again. How can I assist you today?\n","Human: What is 3*3?\n","AI: 3 multiplied by 3 is equal to 9. Is there anything else you would like to know?\n","Human: What is my name?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'Your name is Akar.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["<a id=\"4\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 4. Conversation Token Buffer Memory</b></div>\n","\n","With the conversational token buffer memory, the memory will limit the number of tokens saved. Since a lot of LLM pricing is based on tokens, this maps more directly to the cost of the LLM calls. Let's set the max token limit to 50. Let’s input the following conversation “AI is what? Amazing. Backpropagation is what? Beautiful. Chatbot is what? Charming”"],"metadata":{"id":"V22w0wSUMc3t"}},{"cell_type":"code","source":["!pip install tiktoken\n","from langchain.memory import ConversationTokenBufferMemory\n","\n","memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n","memory.save_context({\"input\": \"AI is what?!\"},\n","                    {\"output\": \"Amazing!\"})\n","memory.save_context({\"input\": \"Backpropagation is what?\"},\n","                    {\"output\": \"Beautiful!\"})\n","memory.save_context({\"input\": \"Chatbots are what?\"},\n","                    {\"output\": \"Charming!\"})\n","\n","memory.load_memory_variables({})\n"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:08:58.95121Z","iopub.execute_input":"2024-05-18T08:08:58.951942Z","iopub.status.idle":"2024-05-18T08:09:15.174882Z","shell.execute_reply.started":"2024-05-18T08:08:58.951903Z","shell.execute_reply":"2024-05-18T08:09:15.173627Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"HQ7AZCpAMc3t","executionInfo":{"status":"ok","timestamp":1716313213614,"user_tz":-120,"elapsed":8836,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"d434b6d7-9a2b-45ca-aeb2-4cfd98bd049e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n","Installing collected packages: tiktoken\n","Successfully installed tiktoken-0.7.0\n"]},{"output_type":"execute_result","data":{"text/plain":["{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":[" If I run this with a high token limit, it has almost the whole conversation. If I increase the token limit to 100, it now has the whole conversation."],"metadata":{"id":"ilK4d7zhMc3t"}},{"cell_type":"code","source":["memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n","memory.save_context({\"input\": \"AI is what?!\"},\n","                    {\"output\": \"Amazing!\"})\n","memory.save_context({\"input\": \"Backpropagation is what?\"},\n","                    {\"output\": \"Beautiful!\"})\n","memory.save_context({\"input\": \"Chatbots are what?\"},\n","                    {\"output\": \"Charming!\"})\n","memory.load_memory_variables({})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:16:04.327712Z","iopub.execute_input":"2024-05-18T08:16:04.328161Z","iopub.status.idle":"2024-05-18T08:16:04.339446Z","shell.execute_reply.started":"2024-05-18T08:16:04.328107Z","shell.execute_reply":"2024-05-18T08:16:04.33838Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"Ejsf0mEMMc3t","executionInfo":{"status":"ok","timestamp":1716313224631,"user_tz":-120,"elapsed":3,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"0f4ad5c0-83b3-4e5f-84aa-6e28ffa2a687"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["If I decrease it to 20, Then, you know, it chops off the earlier parts of this conversation to retain the number of tokens corresponding to the most recent exchanges but subject to not exceeding the token limit."],"metadata":{"id":"Chc8flOqMc3u"}},{"cell_type":"code","source":["memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=20)\n","memory.save_context({\"input\": \"AI is what?!\"},\n","                    {\"output\": \"Amazing!\"})\n","memory.save_context({\"input\": \"Backpropagation is what?\"},\n","                    {\"output\": \"Beautiful!\"})\n","memory.save_context({\"input\": \"Chatbots are what?\"},\n","                    {\"output\": \"Charming!\"})\n","memory.load_memory_variables({})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:16:24.235519Z","iopub.execute_input":"2024-05-18T08:16:24.235941Z","iopub.status.idle":"2024-05-18T08:16:24.246687Z","shell.execute_reply.started":"2024-05-18T08:16:24.235911Z","shell.execute_reply":"2024-05-18T08:16:24.24564Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"PSoWv4XnMc3u","executionInfo":{"status":"ok","timestamp":1716313231244,"user_tz":-120,"elapsed":218,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"1157f7ea-e166-44a3-df9e-cdfdf48e82a5"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'AI: Charming!'}"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["<a id=\"5\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 5. Conversation Summary Memory</b></div>"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:16:31.814248Z","iopub.execute_input":"2024-05-18T08:16:31.814667Z","iopub.status.idle":"2024-05-18T08:16:31.821513Z","shell.execute_reply.started":"2024-05-18T08:16:31.814629Z","shell.execute_reply":"2024-05-18T08:16:31.820015Z"},"id":"TCqozp-SMc3u"}},{"cell_type":"markdown","source":["Finally, there’s one last type of memory we will explore, which is the conversation summary buffer memory. The idea is that instead of limiting the memory to a fixed number of tokens based on the most recent utterances or a fixed number of conversational exchanges, let’s use an LLM to write a summary of the conversation and let that be the memory.\n","\n","So here’s an example where I’m going to create a long string to schedule a meeting with the product team:"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:16:45.634188Z","iopub.execute_input":"2024-05-18T08:16:45.63464Z","iopub.status.idle":"2024-05-18T08:16:45.643295Z","shell.execute_reply.started":"2024-05-18T08:16:45.634571Z","shell.execute_reply":"2024-05-18T08:16:45.641861Z"},"id":"wer8E3sPMc3u"}},{"cell_type":"code","source":["# create a long string\n","schedule = \"There is a meeting at 8am with your product team. \\\n","You will need your powerpoint presentation prepared. \\\n","9am-12pm have time to work on your LangChain \\\n","project which will go quickly because Langchain is such a powerful tool. \\\n","At Noon, lunch at the italian resturant with a customer who is driving \\\n","from over an hour away to meet you to understand the latest in AI. \\\n","Be sure to bring your laptop to show the latest LLM demo.\"\n"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:16:58.535381Z","iopub.execute_input":"2024-05-18T08:16:58.535818Z","iopub.status.idle":"2024-05-18T08:16:58.541404Z","shell.execute_reply.started":"2024-05-18T08:16:58.535785Z","shell.execute_reply":"2024-05-18T08:16:58.540246Z"},"trusted":true,"id":"JAwDbuRmMc3u","executionInfo":{"status":"ok","timestamp":1716313241157,"user_tz":-120,"elapsed":199,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["We will use a conversation summary buffer memory with a max token limit of 400 in this case which is a pretty high token limit, and I’m going to insert in a few conversational terms in which we start with “Hello, what’s up, Not much just hanging, cool, what is on the schedule today, and the response is the long schedule above”.\n","\n","So this memory now has quite a lot of text in it. Let’s take a look at the memory variables. It contains that entire piece of text because 400 tokens were sufficient to store all this text."],"metadata":{"id":"CPa_fCkKMc3u"}},{"cell_type":"code","source":["from langchain.memory import ConversationSummaryBufferMemory\n","\n","memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n","memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})\n","memory.save_context({\"input\": \"What is on the schedule today?\"},\n","                    {\"output\": f\"{schedule}\"})\n","\n","memory.load_memory_variables({})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:17:43.138996Z","iopub.execute_input":"2024-05-18T08:17:43.13939Z","iopub.status.idle":"2024-05-18T08:17:44.447568Z","shell.execute_reply.started":"2024-05-18T08:17:43.139361Z","shell.execute_reply":"2024-05-18T08:17:44.446416Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"bcENdoocMc3u","executionInfo":{"status":"ok","timestamp":1716313247195,"user_tz":-120,"elapsed":2029,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"7adcf3bb-91e6-4d3b-9135-c673404a02cd"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': \"System: The human and AI exchange greetings and discuss the schedule for the day. The AI informs the human of a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of being prepared for the day's events.\"}"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["If we were to reduce the max token limit to 100 tokens, then the conversation summary buffer memory would use an LLM, the OpenAI endpoint in this case to generate a summary of the conversation as shown below:"],"metadata":{"id":"7BHPF5EaMc3y"}},{"cell_type":"code","source":["memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=400)\n","memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})\n","memory.save_context({\"input\": \"What is on the schedule today?\"},\n","                    {\"output\": f\"{schedule}\"})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:18:56.119733Z","iopub.execute_input":"2024-05-18T08:18:56.120132Z","iopub.status.idle":"2024-05-18T08:18:56.127986Z","shell.execute_reply.started":"2024-05-18T08:18:56.120101Z","shell.execute_reply":"2024-05-18T08:18:56.126999Z"},"trusted":true,"id":"6qbEq3tnMc3y","executionInfo":{"status":"ok","timestamp":1716313252099,"user_tz":-120,"elapsed":220,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["If we were to have a conversation, using this LLM, let me create a conversation chain, the same as before. Let’s say that we were to give an input “What would be a good demo to show” and I would set the verbose equals true. The LLM thinks the current conversation has had this discussion so far because that’s the summary of the conversation."],"metadata":{"id":"K90eIGfnMc3y"}},{"cell_type":"code","source":["conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")\n","conversation.predict(input=\"What would be a good demo to show?\")\n"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:19:22.408827Z","iopub.execute_input":"2024-05-18T08:19:22.409234Z","iopub.status.idle":"2024-05-18T08:19:23.730236Z","shell.execute_reply.started":"2024-05-18T08:19:22.409202Z","shell.execute_reply":"2024-05-18T08:19:23.72931Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":368},"id":"bGh20W6dMc3y","executionInfo":{"status":"ok","timestamp":1716313257839,"user_tz":-120,"elapsed":1640,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"}},"outputId":"4a311d8a-b9e2-434a-e83b-4fa6b100bc7e"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hello\n","AI: What's up\n","Human: Not much, just hanging\n","AI: Cool\n","Human: What is on the schedule today?\n","AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n","Human: What would be a good demo to show?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"I would recommend showcasing the Language Model for Language Chains (LLM) demo. It's a cutting-edge AI tool that can generate realistic and coherent text based on a given prompt. It's sure to impress your customer and demonstrate the power of AI technology.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["With the conversation summary buffer memory, the aim is to explicitly store messages up to a specified token limit. In this case, we’re capping explicit storage at 100 tokens, as requested. Any additional content beyond this limit is summarized using CLM, as demonstrated above."],"metadata":{"id":"E-NoG75dMc3z"}},{"cell_type":"markdown","source":["***"],"metadata":{"id":"GdYa_MbnMc3z"}},{"cell_type":"markdown","source":["LangChain also encompasses additional memory types, with one of the most notable being vector data memory. This feature proves particularly potent for those familiar with word embeddings and text embeddings, as it involves storing such embeddings within the vector database. Utilizing this type of vector database memory enables LangChain to retrieve the most relevant blocks of text, enhancing its memory capabilities significantly.\n","\n","When developing applications with LangChain, you have the flexibility to utilize various memory types. This includes leveraging conversation memory, as demonstrated in this article, along with entity memory to recall specific individuals.\n","\n","This approach allows for the retention of both a summarized version of the conversation and explicit details about key individuals involved. Additionally, developers often opt to store the entire conversation in a conventional database, such as a key-value store or SQL database.\n","\n"," This enables easy reference back to the conversation for auditing purposes or further system enhancements. So, these memory types offer a robust framework for building your applications."],"metadata":{"id":"JeQtS_jHMc3z"}},{"cell_type":"markdown","source":["# <div style=\"box-shadow: rgba(240, 46, 170, 0.4) -5px 5px inset, rgba(240, 46, 170, 0.3) -10px 10px inset, rgba(240, 46, 170, 0.2) -15px 15px inset, rgba(240, 46, 170, 0.1) -20px 20px inset, rgba(240, 46, 170, 0.05) -25px 25px inset; padding:20px; font-size:30px; font-family: consolas; display:fill; border-radius:15px; color: rgba(240, 46, 170, 0.7)\"> <b> ༼⁠ ⁠つ⁠ ⁠◕⁠‿⁠◕⁠ ⁠༽⁠つ Thank You!</b></div>\n","\n","<p style=\"font-family:verdana; color:rgb(34, 34, 34); font-family: consolas; font-size: 16px;\"> 💌 Thank you for taking the time to read through my notebook. I hope you found it interesting and informative. If you have any feedback or suggestions for improvement, please don't hesitate to let me know in the comments. <br><br> 🚀 If you liked this notebook, please consider upvoting it so that others can discover it too. Your support means a lot to me, and it helps to motivate me to create more content in the future. <br><br> ❤️ Once again, thank you for your support, and I hope to see you again soon!</p>"],"metadata":{"id":"3M5V7fztMc3z"}}]}