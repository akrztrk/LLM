{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"toc_visible":true,"machine_shape":"hm"}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">  Hands-On LangChain for LLM Applications Development: Chatbots Memory </center>\n","\n","***\n","\n","\n","When interacting with language models, such as Chatbots, the absence of memory poses a significant hurdle in creating natural and seamless conversations. Users expect continuity and context retention, which traditional models lack. This limitation becomes particularly evident in applications where ongoing dialogue is crucial for user engagement and satisfaction.\n","\n","LangChain offers robust solutions to address this challenge. Memory, in this context, refers to the ability of the language model to remember previous parts of a conversation and use that information to inform subsequent interactions. By incorporating memory into the model‚Äôs architecture, LangChain enables Chatbots and similar applications to maintain a conversational flow that mimics human-like dialogue.\n","\n","LangChain‚Äôs memory capabilities extend beyond mere recall of past interactions. It encompasses sophisticated mechanisms for storing, organizing, and retrieving relevant information, ensuring that the Chatbot can respond appropriately based on the context of the conversation. This not only enhances the user experience but also enables the Chatbot to provide more accurate and relevant responses over time.\n","\n","#### <a id=\"top\"></a>\n","# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b>Table of contents</b></div>\n","\n","<div style=\"background-color: rgba(60, 121, 245, 0.03); padding:30px; font-size:15px; font-family: consolas;\">\n","<ul>\n","    <li><a href=\"#1\" target=\"_self\" rel=\" noreferrer nofollow\">1. Setting Up Working Environment & Getting Starting </a> </li>\n","    <li><a href=\"#2\" target=\"_self\" rel=\" noreferrer nofollow\">2. Conversation Buffer Memory </a></li>\n","    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">3. Conversation Buffer Window Memory </a></li>\n","    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">4. Conversation Token Buffer Memory </a></li>\n","    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">5. Conversation Summary Memory </a></li>\n","</ul>\n","</div>\n","\n","***\n","\n"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"KzKMrPJ5Mc3k"}},{"cell_type":"markdown","source":["<a id=\"1\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 1. Setting Up Working Environment & Getting¬†Started </b></div>\n","\n","To get started we are going to import OS, import OpenAI, and load my OpenAI secret key. If you‚Äôre running this locally, and you don‚Äôt have OpenAI installed yet, you might need to run pip to install OpenAI."],"metadata":{"id":"Hf0gENmoMc3n"}},{"cell_type":"code","source":["!pip install langchain\n","!pip install langchain_community\n","!pip install openai"],"metadata":{"id":"FWtLhsN5NHDh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import openai\n","from openai import OpenAI\n","from getpass import getpass\n","\n","OPENAI_API_KEY =  getpass('Please enter your open_api_key:')\n","api_key = {\n","    \"OPENAI_API_KEY\":OPENAI_API_KEY\n","}\n","locals().update(api_key)\n","os.environ.update(api_key)\n","\n","llm_model = \"gpt-3.5-turbo\""],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:46:05.922131Z","iopub.execute_input":"2024-05-18T07:46:05.923081Z","iopub.status.idle":"2024-05-18T07:46:52.902329Z","shell.execute_reply.started":"2024-05-18T07:46:05.923045Z","shell.execute_reply":"2024-05-18T07:46:52.901317Z"},"trusted":true,"id":"FEEF-FLYMc3o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"2\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 2. Conversation Buffer Memory </b></div>\n","\n","\n","Let‚Äôs start with a motivating example for memory, using LangChain to manage a chat or a chatbot conversation. I am going to set the LLM as a chat interface of OpenAI with a temperature equal to 0. We will use the memory as a ConversationBufferMemory and then build a conversation chain."],"metadata":{"id":"uvAK4CLlMc3p"}},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","from langchain.chains import ConversationChain\n","from langchain.memory import ConversationBufferMemory\n","\n","llm = ChatOpenAI(temperature=0.0, model=llm_model, openai_api_key=OPENAI_API_KEY)\n","memory = ConversationBufferMemory()\n","conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:50:39.403443Z","iopub.execute_input":"2024-05-18T07:50:39.40414Z","iopub.status.idle":"2024-05-18T07:50:39.439666Z","shell.execute_reply.started":"2024-05-18T07:50:39.404105Z","shell.execute_reply":"2024-05-18T07:50:39.438752Z"},"trusted":true,"id":"M_kE1ZKEMc3p","executionInfo":{"status":"ok","timestamp":1716312746477,"user_tz":-120,"elapsed":204,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Let's start a conversation using conversation.predict() and the given input will be ‚ÄúHi, my name is Youssef‚Äù and let‚Äôs see the response."],"metadata":{"id":"nvNJNlXbMc3q"}},{"cell_type":"code","source":["conversation.predict(input=\"Hi, my name is Akar\")"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:51:04.294733Z","iopub.execute_input":"2024-05-18T07:51:04.29509Z","iopub.status.idle":"2024-05-18T07:51:05.200851Z","shell.execute_reply.started":"2024-05-18T07:51:04.295063Z","shell.execute_reply":"2024-05-18T07:51:05.199826Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":264},"id":"54aPboktMc3q","executionInfo":{"status":"ok","timestamp":1716312757141,"user_tz":-120,"elapsed":1347,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"5c73fb93-c3d3-42ca-d3a2-f4dc5fb13d1a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","\n","Human: Hi, my name is Akar\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Hello Akar! It's nice to meet you. How can I assist you today?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Then, let‚Äôs ask it, what is 1 plus 1? 1 plus 1 is 2, and then ask it again, you know, what‚Äôs my name?"],"metadata":{"id":"L9ytGQBbMc3q"}},{"cell_type":"code","source":["conversation.predict(input=\"What is 3*3?\")"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:51:49.18206Z","iopub.execute_input":"2024-05-18T07:51:49.182827Z","iopub.status.idle":"2024-05-18T07:51:50.099916Z","shell.execute_reply.started":"2024-05-18T07:51:49.182791Z","shell.execute_reply":"2024-05-18T07:51:50.09882Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"8oo-TjPxMc3q","executionInfo":{"status":"ok","timestamp":1716312917192,"user_tz":-120,"elapsed":1093,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"f81f77c5-b945-4e95-ee58-20b46e183d1d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you. How can I assist you today?\n","Human: What is 3*3?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'3 multiplied by 3 is equal to 9. Is there anything else you would like to know?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Now we will ask it again, ‚ÄúWhat‚Äôs my name?‚Äù and you can see its response ‚ÄúYour name is Youssef, as you mentioned earlier‚Äù."],"metadata":{"id":"JtvHqm-jMc3r"}},{"cell_type":"code","source":["conversation.predict(input=\"What is my name?\")\n"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:52:13.890842Z","iopub.execute_input":"2024-05-18T07:52:13.89123Z","iopub.status.idle":"2024-05-18T07:52:15.70993Z","shell.execute_reply.started":"2024-05-18T07:52:13.891198Z","shell.execute_reply":"2024-05-18T07:52:15.708913Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":316},"id":"DVaVIvC0Mc3r","executionInfo":{"status":"ok","timestamp":1716312925430,"user_tz":-120,"elapsed":780,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"f0ef0bf0-f853-4a4e-f5fc-0c992b49419d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you. How can I assist you today?\n","Human: What is 3*3?\n","AI: 3 multiplied by 3 is equal to 9. Is there anything else you would like to know?\n","Human: What is my name?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'Your name is Akar.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["So this is a prompt that LangChain has generated to have the system have a hopeful and friendly conversation with you. When you execute this on the second and third parts of the conversations, it keeps the prompt as follows.\n","\n","Since I have used the memory variable to store the memory. So if I were to print memory.buffer, it has stored the conversation so far."],"metadata":{"id":"MWlLKxZ1Mc3r"}},{"cell_type":"code","source":["print(memory.buffer)\n"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:52:54.399121Z","iopub.execute_input":"2024-05-18T07:52:54.399624Z","iopub.status.idle":"2024-05-18T07:52:54.405435Z","shell.execute_reply.started":"2024-05-18T07:52:54.399569Z","shell.execute_reply":"2024-05-18T07:52:54.404319Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"MCbJhYstMc3r","executionInfo":{"status":"ok","timestamp":1716312948778,"user_tz":-120,"elapsed":249,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"4ec3e161-cfa9-4eb0-9ccc-f6b90cedd548"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you. How can I assist you today?\n","Human: What is 3*3?\n","AI: 3 multiplied by 3 is equal to 9. Is there anything else you would like to know?\n","Human: What is my name?\n","AI: Your name is Akar.\n"]}]},{"cell_type":"markdown","source":["You can also print this out using the memory.loadMemoryVariables({}). The curly braces here are an empty dictionary. There are some more advanced features that you can use with a more sophisticated input, but it is not covered in this article but you can find it in LangChain documentation. So don‚Äôt worry about why there‚Äôs an empty curly braces here. But this is what LangChain has remembered in the memory of the conversation so far. It‚Äôs just everything that the AI or the human has said."],"metadata":{"id":"QS53AiJYMc3r"}},{"cell_type":"code","source":["memory.load_memory_variables({})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:53:29.799839Z","iopub.execute_input":"2024-05-18T07:53:29.800213Z","iopub.status.idle":"2024-05-18T07:53:29.807565Z","shell.execute_reply.started":"2024-05-18T07:53:29.800184Z","shell.execute_reply":"2024-05-18T07:53:29.806446Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"NskDm10AMc3s","executionInfo":{"status":"ok","timestamp":1716312956630,"user_tz":-120,"elapsed":245,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"5393ea09-ba33-4d9d-cb83-cd145c76b8ae"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': \"Human: Hi, my name is Akar\\nAI: Hello Akar! It's nice to meet you. How can I assist you today?\\nHuman: What is 3*3?\\nAI: 3 multiplied by 3 is equal to 9. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Akar.\"}"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["The way that LangChain is storing the conversation is with this ConversationBufferMemory. If I were to use the ConversationBufferMemory to specify a couple of inputs and outputs, this is how you add new things to the memory if you wish to do so explicitly."],"metadata":{"id":"uArqPkhpMc3s"}},{"cell_type":"code","source":["memory = ConversationBufferMemory()\n","memory.save_context({\"input\": \"Hi\"},\n","                    {\"output\": \"What's up\"})\n"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T07:54:16.2189Z","iopub.execute_input":"2024-05-18T07:54:16.2193Z","iopub.status.idle":"2024-05-18T07:54:16.224751Z","shell.execute_reply.started":"2024-05-18T07:54:16.219269Z","shell.execute_reply":"2024-05-18T07:54:16.223648Z"},"trusted":true,"id":"cH6AlTbAMc3s","executionInfo":{"status":"ok","timestamp":1716312962664,"user_tz":-120,"elapsed":207,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Memory.saveContext says, hi, what‚Äôs up? I know this is not the most exciting conversation, but I wanted to give a short example. And with that, this is what the status of the memory is. Once again, let me show the memory variables. Now, if you want to add additional data to the memory, you can keep on saving additional context. So, the conversation goes on, not much, just hanging, cool."],"metadata":{"id":"rDhkpzg_Mc3s"}},{"cell_type":"code","source":["print(memory.buffer)"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:05:07.071108Z","iopub.execute_input":"2024-05-18T08:05:07.071515Z","iopub.status.idle":"2024-05-18T08:05:07.077079Z","shell.execute_reply.started":"2024-05-18T08:05:07.071486Z","shell.execute_reply":"2024-05-18T08:05:07.075969Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"LwcB3CmzMc3s","executionInfo":{"status":"ok","timestamp":1716312965671,"user_tz":-120,"elapsed":202,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"4407887d-185d-4ebd-87a4-0a3f8fd8e30f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Human: Hi\n","AI: What's up\n"]}]},{"cell_type":"markdown","source":["When you use a large language model for a chat conversation, the large language model itself is stateless. The language model itself does not remember the conversation you‚Äôve had so far. Therefore each call to the API endpoint is independent. Chatbots have memory only because there‚Äôs usually rapid code that provides the full conversation that‚Äôs been had so far as context to the LLM. Therefore the memory can store explicitly the terms or the utterances so far. Hi, my name is Youssef. Hello, it‚Äôs just nice to meet you and so on.\n","\n","This memory storage is used as input or additional context to the LLM so that it can generate output as if it‚Äôs just having the next conversational turn, knowing what‚Äôs been said before.\n","\n","As the conversation becomes long, the amount of memory needed becomes long, and the cost of sending a lot of tokens to the LLM, which usually charges based on the number of tokens it needs to process, will also become more expensive.\n","\n","So LangChain provides several convenient kinds of memory to store and accumulate the conversation. So far, we‚Äôve been looking at the ConversationBufferMemory. Let‚Äôs look at a different type of memory."],"metadata":{"id":"YuUCc7dQMc3s"}},{"cell_type":"markdown","source":["<a id=\"3\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 3. Conversation Buffer Window Memory</b></div>\n","\n","Let's start with importing the conversation buffer window memory that only keeps a window of memory. If I set memory to conversational buffer window memory with k equals one, the variable k =1 specifies that I want to remember just one conversational exchange. That is one utterance from me and one utterance from a chatbot.\n","\n"],"metadata":{"id":"cyRdCsxmMc3s"}},{"cell_type":"code","source":["from langchain.memory import ConversationBufferWindowMemory\n","memory = ConversationBufferWindowMemory(k=1)\n","memory.load_memory_variables({})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:05:53.796262Z","iopub.execute_input":"2024-05-18T08:05:53.796637Z","iopub.status.idle":"2024-05-18T08:05:53.80395Z","shell.execute_reply.started":"2024-05-18T08:05:53.796597Z","shell.execute_reply":"2024-05-18T08:05:53.8031Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"bEEpPsgIMc3s","executionInfo":{"status":"ok","timestamp":1716313172282,"user_tz":-120,"elapsed":255,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"7f048189-0c07-4e03-bda0-23728cc9643c"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': ''}"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["If I were to have it save context, ‚ÄúHi, what‚Äôs up, not much, just hanging‚Äù. If I were to look at memory.load_memory_variables({}), it only remembers the most recent utterance as you can see below."],"metadata":{"id":"XmeGwdFOMc3t"}},{"cell_type":"code","source":["memory.save_context({\"input\": \"Hi\"},\n","                    {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})\n","memory.load_memory_variables({})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:06:21.292913Z","iopub.execute_input":"2024-05-18T08:06:21.293337Z","iopub.status.idle":"2024-05-18T08:06:21.301785Z","shell.execute_reply.started":"2024-05-18T08:06:21.293304Z","shell.execute_reply":"2024-05-18T08:06:21.300502Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"f3mGvu2ZMc3t","executionInfo":{"status":"ok","timestamp":1716313175829,"user_tz":-120,"elapsed":291,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"dc6702b6-2bf5-4df0-d449-2227d1c4c9ee"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'Human: Not much, just hanging\\nAI: Cool'}"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["It only returns the recent conversation because k was equal to one. So this is a nice feature because it lets you keep track of just the most recent few conversational terms. In practice, you probably won‚Äôt use this with k equals one. You use this with k set to a larger number.\n","\n"," If I were to rerun the conversation that we had above, we would say ‚ÄúHi, my name is Youssef‚Äù. Then we will say ‚ÄúWhat is 1 plus 1?‚Äù. Finally, I will ask it ‚ÄúWhat is my name?‚Äù. Because we set k equals 1, it only remembers the last exchange versus what is 1 plus 1? The answer is 1 plus 1 equals 2, and it‚Äôs forgotten this early exchange which is now, now says, sorry, don‚Äôt have access to that information."],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:06:31.602687Z","iopub.execute_input":"2024-05-18T08:06:31.60337Z","iopub.status.idle":"2024-05-18T08:06:31.61302Z","shell.execute_reply.started":"2024-05-18T08:06:31.603334Z","shell.execute_reply":"2024-05-18T08:06:31.611658Z"},"id":"5o24uBNCMc3t"}},{"cell_type":"code","source":["conversation.predict(input=\"Hi, my name is Akar\")"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:07:44.147044Z","iopub.execute_input":"2024-05-18T08:07:44.147454Z","iopub.status.idle":"2024-05-18T08:07:44.863981Z","shell.execute_reply.started":"2024-05-18T08:07:44.14742Z","shell.execute_reply":"2024-05-18T08:07:44.862943Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":351},"id":"U5VrjYNAMc3t","executionInfo":{"status":"ok","timestamp":1716313191106,"user_tz":-120,"elapsed":1035,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"2fa8cb8d-ad91-463c-bc36-69e6c3d44c50"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you. How can I assist you today?\n","Human: What is 3*3?\n","AI: 3 multiplied by 3 is equal to 9. Is there anything else you would like to know?\n","Human: What is my name?\n","AI: Your name is Akar.\n","Human: Hi, my name is Akar\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Hello Akar! It's nice to meet you again. How can I assist you today?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["conversation.predict(input=\"What is 3*3?\")"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:07:52.683022Z","iopub.execute_input":"2024-05-18T08:07:52.683442Z","iopub.status.idle":"2024-05-18T08:07:53.424795Z","shell.execute_reply.started":"2024-05-18T08:07:52.68341Z","shell.execute_reply":"2024-05-18T08:07:53.423629Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":385},"id":"XUkpWnV8Mc3t","executionInfo":{"status":"ok","timestamp":1716313194329,"user_tz":-120,"elapsed":830,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"ee24d285-bb42-4216-c17b-f952ddc61f57"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you. How can I assist you today?\n","Human: What is 3*3?\n","AI: 3 multiplied by 3 is equal to 9. Is there anything else you would like to know?\n","Human: What is my name?\n","AI: Your name is Akar.\n","Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you again. How can I assist you today?\n","Human: What is 3*3?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'3 multiplied by 3 is equal to 9. Is there anything else you would like to know?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["conversation.predict(input=\"What is my name?\")"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:08:01.124116Z","iopub.execute_input":"2024-05-18T08:08:01.124961Z","iopub.status.idle":"2024-05-18T08:08:01.951135Z","shell.execute_reply.started":"2024-05-18T08:08:01.124918Z","shell.execute_reply":"2024-05-18T08:08:01.95015Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":420},"id":"p9K0cmwdMc3t","executionInfo":{"status":"ok","timestamp":1716313197068,"user_tz":-120,"elapsed":820,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"c33b9cac-4ad5-4b48-ef69-e735d93f5ca2"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you. How can I assist you today?\n","Human: What is 3*3?\n","AI: 3 multiplied by 3 is equal to 9. Is there anything else you would like to know?\n","Human: What is my name?\n","AI: Your name is Akar.\n","Human: Hi, my name is Akar\n","AI: Hello Akar! It's nice to meet you again. How can I assist you today?\n","Human: What is 3*3?\n","AI: 3 multiplied by 3 is equal to 9. Is there anything else you would like to know?\n","Human: What is my name?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'Your name is Akar.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["<a id=\"4\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 4. Conversation Token Buffer Memory</b></div>\n","\n","With the conversational token buffer memory, the memory will limit the number of tokens saved. Since a lot of LLM pricing is based on tokens, this maps more directly to the cost of the LLM calls. Let's set the max token limit to 50. Let‚Äôs input the following conversation ‚ÄúAI is what? Amazing. Backpropagation is what? Beautiful. Chatbot is what? Charming‚Äù"],"metadata":{"id":"V22w0wSUMc3t"}},{"cell_type":"code","source":["!pip install tiktoken\n","from langchain.memory import ConversationTokenBufferMemory\n","\n","memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n","memory.save_context({\"input\": \"AI is what?!\"},\n","                    {\"output\": \"Amazing!\"})\n","memory.save_context({\"input\": \"Backpropagation is what?\"},\n","                    {\"output\": \"Beautiful!\"})\n","memory.save_context({\"input\": \"Chatbots are what?\"},\n","                    {\"output\": \"Charming!\"})\n","\n","memory.load_memory_variables({})\n"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:08:58.95121Z","iopub.execute_input":"2024-05-18T08:08:58.951942Z","iopub.status.idle":"2024-05-18T08:09:15.174882Z","shell.execute_reply.started":"2024-05-18T08:08:58.951903Z","shell.execute_reply":"2024-05-18T08:09:15.173627Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"HQ7AZCpAMc3t","executionInfo":{"status":"ok","timestamp":1716313213614,"user_tz":-120,"elapsed":8836,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"d434b6d7-9a2b-45ca-aeb2-4cfd98bd049e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n","Installing collected packages: tiktoken\n","Successfully installed tiktoken-0.7.0\n"]},{"output_type":"execute_result","data":{"text/plain":["{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":[" If I run this with a high token limit, it has almost the whole conversation. If I increase the token limit to 100, it now has the whole conversation."],"metadata":{"id":"ilK4d7zhMc3t"}},{"cell_type":"code","source":["memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n","memory.save_context({\"input\": \"AI is what?!\"},\n","                    {\"output\": \"Amazing!\"})\n","memory.save_context({\"input\": \"Backpropagation is what?\"},\n","                    {\"output\": \"Beautiful!\"})\n","memory.save_context({\"input\": \"Chatbots are what?\"},\n","                    {\"output\": \"Charming!\"})\n","memory.load_memory_variables({})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:16:04.327712Z","iopub.execute_input":"2024-05-18T08:16:04.328161Z","iopub.status.idle":"2024-05-18T08:16:04.339446Z","shell.execute_reply.started":"2024-05-18T08:16:04.328107Z","shell.execute_reply":"2024-05-18T08:16:04.33838Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"Ejsf0mEMMc3t","executionInfo":{"status":"ok","timestamp":1716313224631,"user_tz":-120,"elapsed":3,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"0f4ad5c0-83b3-4e5f-84aa-6e28ffa2a687"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["If I decrease it to 20, Then, you know, it chops off the earlier parts of this conversation to retain the number of tokens corresponding to the most recent exchanges but subject to not exceeding the token limit."],"metadata":{"id":"Chc8flOqMc3u"}},{"cell_type":"code","source":["memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=20)\n","memory.save_context({\"input\": \"AI is what?!\"},\n","                    {\"output\": \"Amazing!\"})\n","memory.save_context({\"input\": \"Backpropagation is what?\"},\n","                    {\"output\": \"Beautiful!\"})\n","memory.save_context({\"input\": \"Chatbots are what?\"},\n","                    {\"output\": \"Charming!\"})\n","memory.load_memory_variables({})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:16:24.235519Z","iopub.execute_input":"2024-05-18T08:16:24.235941Z","iopub.status.idle":"2024-05-18T08:16:24.246687Z","shell.execute_reply.started":"2024-05-18T08:16:24.235911Z","shell.execute_reply":"2024-05-18T08:16:24.24564Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"PSoWv4XnMc3u","executionInfo":{"status":"ok","timestamp":1716313231244,"user_tz":-120,"elapsed":218,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"1157f7ea-e166-44a3-df9e-cdfdf48e82a5"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'AI: Charming!'}"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["<a id=\"5\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 5. Conversation Summary Memory</b></div>"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:16:31.814248Z","iopub.execute_input":"2024-05-18T08:16:31.814667Z","iopub.status.idle":"2024-05-18T08:16:31.821513Z","shell.execute_reply.started":"2024-05-18T08:16:31.814629Z","shell.execute_reply":"2024-05-18T08:16:31.820015Z"},"id":"TCqozp-SMc3u"}},{"cell_type":"markdown","source":["Finally, there‚Äôs one last type of memory we will explore, which is the conversation summary buffer memory. The idea is that instead of limiting the memory to a fixed number of tokens based on the most recent utterances or a fixed number of conversational exchanges, let‚Äôs use an LLM to write a summary of the conversation and let that be the memory.\n","\n","So here‚Äôs an example where I‚Äôm going to create a long string to schedule a meeting with the product team:"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:16:45.634188Z","iopub.execute_input":"2024-05-18T08:16:45.63464Z","iopub.status.idle":"2024-05-18T08:16:45.643295Z","shell.execute_reply.started":"2024-05-18T08:16:45.634571Z","shell.execute_reply":"2024-05-18T08:16:45.641861Z"},"id":"wer8E3sPMc3u"}},{"cell_type":"code","source":["# create a long string\n","schedule = \"There is a meeting at 8am with your product team. \\\n","You will need your powerpoint presentation prepared. \\\n","9am-12pm have time to work on your LangChain \\\n","project which will go quickly because Langchain is such a powerful tool. \\\n","At Noon, lunch at the italian resturant with a customer who is driving \\\n","from over an hour away to meet you to understand the latest in AI. \\\n","Be sure to bring your laptop to show the latest LLM demo.\"\n"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:16:58.535381Z","iopub.execute_input":"2024-05-18T08:16:58.535818Z","iopub.status.idle":"2024-05-18T08:16:58.541404Z","shell.execute_reply.started":"2024-05-18T08:16:58.535785Z","shell.execute_reply":"2024-05-18T08:16:58.540246Z"},"trusted":true,"id":"JAwDbuRmMc3u","executionInfo":{"status":"ok","timestamp":1716313241157,"user_tz":-120,"elapsed":199,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["We will use a conversation summary buffer memory with a max token limit of 400 in this case which is a pretty high token limit, and I‚Äôm going to insert in a few conversational terms in which we start with ‚ÄúHello, what‚Äôs up, Not much just hanging, cool, what is on the schedule today, and the response is the long schedule above‚Äù.\n","\n","So this memory now has quite a lot of text in it. Let‚Äôs take a look at the memory variables. It contains that entire piece of text because 400 tokens were sufficient to store all this text."],"metadata":{"id":"CPa_fCkKMc3u"}},{"cell_type":"code","source":["from langchain.memory import ConversationSummaryBufferMemory\n","\n","memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n","memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})\n","memory.save_context({\"input\": \"What is on the schedule today?\"},\n","                    {\"output\": f\"{schedule}\"})\n","\n","memory.load_memory_variables({})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:17:43.138996Z","iopub.execute_input":"2024-05-18T08:17:43.13939Z","iopub.status.idle":"2024-05-18T08:17:44.447568Z","shell.execute_reply.started":"2024-05-18T08:17:43.139361Z","shell.execute_reply":"2024-05-18T08:17:44.446416Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"bcENdoocMc3u","executionInfo":{"status":"ok","timestamp":1716313247195,"user_tz":-120,"elapsed":2029,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"7adcf3bb-91e6-4d3b-9135-c673404a02cd"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': \"System: The human and AI exchange greetings and discuss the schedule for the day. The AI informs the human of a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of being prepared for the day's events.\"}"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["If we were to reduce the max token limit to 100 tokens, then the conversation summary buffer memory would use an LLM, the OpenAI endpoint in this case to generate a summary of the conversation as shown below:"],"metadata":{"id":"7BHPF5EaMc3y"}},{"cell_type":"code","source":["memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=400)\n","memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})\n","memory.save_context({\"input\": \"What is on the schedule today?\"},\n","                    {\"output\": f\"{schedule}\"})"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:18:56.119733Z","iopub.execute_input":"2024-05-18T08:18:56.120132Z","iopub.status.idle":"2024-05-18T08:18:56.127986Z","shell.execute_reply.started":"2024-05-18T08:18:56.120101Z","shell.execute_reply":"2024-05-18T08:18:56.126999Z"},"trusted":true,"id":"6qbEq3tnMc3y","executionInfo":{"status":"ok","timestamp":1716313252099,"user_tz":-120,"elapsed":220,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["If we were to have a conversation, using this LLM, let me create a conversation chain, the same as before. Let‚Äôs say that we were to give an input ‚ÄúWhat would be a good demo to show‚Äù and I would set the verbose equals true. The LLM thinks the current conversation has had this discussion so far because that‚Äôs the summary of the conversation."],"metadata":{"id":"K90eIGfnMc3y"}},{"cell_type":"code","source":["conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")\n","conversation.predict(input=\"What would be a good demo to show?\")\n"],"metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:19:22.408827Z","iopub.execute_input":"2024-05-18T08:19:22.409234Z","iopub.status.idle":"2024-05-18T08:19:23.730236Z","shell.execute_reply.started":"2024-05-18T08:19:22.409202Z","shell.execute_reply":"2024-05-18T08:19:23.72931Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":368},"id":"bGh20W6dMc3y","executionInfo":{"status":"ok","timestamp":1716313257839,"user_tz":-120,"elapsed":1640,"user":{"displayName":"Akar √ñzt√ºrk","userId":"07602883577262601067"}},"outputId":"4a311d8a-b9e2-434a-e83b-4fa6b100bc7e"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hello\n","AI: What's up\n","Human: Not much, just hanging\n","AI: Cool\n","Human: What is on the schedule today?\n","AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n","Human: What would be a good demo to show?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"I would recommend showcasing the Language Model for Language Chains (LLM) demo. It's a cutting-edge AI tool that can generate realistic and coherent text based on a given prompt. It's sure to impress your customer and demonstrate the power of AI technology.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["With the conversation summary buffer memory, the aim is to explicitly store messages up to a specified token limit. In this case, we‚Äôre capping explicit storage at 100 tokens, as requested. Any additional content beyond this limit is summarized using CLM, as demonstrated above."],"metadata":{"id":"E-NoG75dMc3z"}},{"cell_type":"markdown","source":["***"],"metadata":{"id":"GdYa_MbnMc3z"}},{"cell_type":"markdown","source":["LangChain also encompasses additional memory types, with one of the most notable being vector data memory. This feature proves particularly potent for those familiar with word embeddings and text embeddings, as it involves storing such embeddings within the vector database. Utilizing this type of vector database memory enables LangChain to retrieve the most relevant blocks of text, enhancing its memory capabilities significantly.\n","\n","When developing applications with LangChain, you have the flexibility to utilize various memory types. This includes leveraging conversation memory, as demonstrated in this article, along with entity memory to recall specific individuals.\n","\n","This approach allows for the retention of both a summarized version of the conversation and explicit details about key individuals involved. Additionally, developers often opt to store the entire conversation in a conventional database, such as a key-value store or SQL database.\n","\n"," This enables easy reference back to the conversation for auditing purposes or further system enhancements. So, these memory types offer a robust framework for building your applications."],"metadata":{"id":"JeQtS_jHMc3z"}},{"cell_type":"markdown","source":["# <div style=\"box-shadow: rgba(240, 46, 170, 0.4) -5px 5px inset, rgba(240, 46, 170, 0.3) -10px 10px inset, rgba(240, 46, 170, 0.2) -15px 15px inset, rgba(240, 46, 170, 0.1) -20px 20px inset, rgba(240, 46, 170, 0.05) -25px 25px inset; padding:20px; font-size:30px; font-family: consolas; display:fill; border-radius:15px; color: rgba(240, 46, 170, 0.7)\"> <b> ‡ºº‚Å† ‚Å†„Å§‚Å† ‚Å†‚óï‚Å†‚Äø‚Å†‚óï‚Å† ‚Å†‡ºΩ‚Å†„Å§ Thank You!</b></div>\n","\n","<p style=\"font-family:verdana; color:rgb(34, 34, 34); font-family: consolas; font-size: 16px;\"> üíå Thank you for taking the time to read through my notebook. I hope you found it interesting and informative. If you have any feedback or suggestions for improvement, please don't hesitate to let me know in the comments. <br><br> üöÄ If you liked this notebook, please consider upvoting it so that others can discover it too. Your support means a lot to me, and it helps to motivate me to create more content in the future. <br><br> ‚ù§Ô∏è Once again, thank you for your support, and I hope to see you again soon!</p>"],"metadata":{"id":"3M5V7fztMc3z"}}]}